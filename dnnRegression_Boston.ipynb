{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b17e2f3",
   "metadata": {},
   "source": [
    "+ [download dataset](https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset/data)\n",
    "+ items:\n",
    "    + CRIM - per capita crime rate by town\n",
    "    + ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    + INDUS - proportion of non-retail business acres per town.\n",
    "    + CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "    + NOX - nitric oxides concentration (parts per 10 million)\n",
    "    + RM - average number of rooms per dwelling\n",
    "    + AGE - proportion of owner-occupied units built prior to 1940\n",
    "    + DIS - weighted distances to five Boston employment centres\n",
    "    + RAD - index of accessibility to radial highways\n",
    "    + TAX - full-value property-tax rate per \\$10,000\n",
    "    + PTRATIO - pupil-teacher ratio by town\n",
    "    + B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "    + LSTAT - % lower status of the population\n",
    "    + MEDV - Median value of owner-occupied homes in $1000's\n",
    "+ 14 inputs -> 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f18ba282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math, time, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c117857",
   "metadata": {},
   "source": [
    "### Observation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9dbb18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.00632  18.00   2.310  0  0.5380  6.5750  65.20  4.0900   1  296.0  15.30 396.90   4.98  24.00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.02731   0.00   7.070  0  0.4690  6.4210  78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02729   0.00   7.070  0  0.4690  7.1850  61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03237   0.00   2.180  0  0.4580  6.9980  45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.06905   0.00   2.180  0  0.4580  7.1470  54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.02985   0.00   2.180  0  0.4580  6.4300  58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.06263   0.00  11.930  0  0.5730  6.5930  69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.04527   0.00  11.930  0  0.5730  6.1200  76...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.06076   0.00  11.930  0  0.5730  6.9760  91...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.10959   0.00  11.930  0  0.5730  6.7940  89...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.04741   0.00  11.930  0  0.5730  6.0300  80...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.00632  18.00   2.310  0  0.5380  6.5750  65.20  4.0900   1  296.0  15.30 396.90   4.98  24.00\n",
       "0     0.02731   0.00   7.070  0  0.4690  6.4210  78...                                              \n",
       "1     0.02729   0.00   7.070  0  0.4690  7.1850  61...                                              \n",
       "2     0.03237   0.00   2.180  0  0.4580  6.9980  45...                                              \n",
       "3     0.06905   0.00   2.180  0  0.4580  7.1470  54...                                              \n",
       "4     0.02985   0.00   2.180  0  0.4580  6.4300  58...                                              \n",
       "..                                                 ...                                              \n",
       "500   0.06263   0.00  11.930  0  0.5730  6.5930  69...                                              \n",
       "501   0.04527   0.00  11.930  0  0.5730  6.1200  76...                                              \n",
       "502   0.06076   0.00  11.930  0  0.5730  6.9760  91...                                              \n",
       "503   0.10959   0.00  11.930  0  0.5730  6.7940  89...                                              \n",
       "504   0.04741   0.00  11.930  0  0.5730  6.0300  80...                                              \n",
       "\n",
       "[505 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./housing.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2cdada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2    3      4      5     6       7    8      9     10  \\\n",
       "0    0.00632  18.0   2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
       "1    0.02731   0.0   7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
       "2    0.02729   0.0   7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
       "3    0.03237   0.0   2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
       "4    0.06905   0.0   2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
       "..       ...   ...    ...  ...    ...    ...   ...     ...  ...    ...   ...   \n",
       "501  0.06263   0.0  11.93  0.0  0.573  6.593  69.1  2.4786  1.0  273.0  21.0   \n",
       "502  0.04527   0.0  11.93  0.0  0.573  6.120  76.7  2.2875  1.0  273.0  21.0   \n",
       "503  0.06076   0.0  11.93  0.0  0.573  6.976  91.0  2.1675  1.0  273.0  21.0   \n",
       "504  0.10959   0.0  11.93  0.0  0.573  6.794  89.3  2.3889  1.0  273.0  21.0   \n",
       "505  0.04741   0.0  11.93  0.0  0.573  6.030  80.8  2.5050  1.0  273.0  21.0   \n",
       "\n",
       "         11    12    13  \n",
       "0    396.90  4.98  24.0  \n",
       "1    396.90  9.14  21.6  \n",
       "2    392.83  4.03  34.7  \n",
       "3    394.63  2.94  33.4  \n",
       "4    396.90  5.33  36.2  \n",
       "..      ...   ...   ...  \n",
       "501  391.99  9.67  22.4  \n",
       "502  396.90  9.08  20.6  \n",
       "503  396.90  5.64  23.9  \n",
       "504  393.45  6.48  22.0  \n",
       "505  396.90  7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "with open(\"./housing.csv\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        L = list(filter(lambda s:s, line.split(\" \")))\n",
    "        L = list(map(lambda s:float(s.replace('\\n','')),L))\n",
    "        assert len(L)==14\n",
    "        data.append(L)\n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a495cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "               12          13  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbce128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "499\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2    3      4      5     6       7    8      9     10  \\\n",
       "0    0.00632  18.0   2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
       "1    0.02731   0.0   7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
       "2    0.02729   0.0   7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
       "3    0.03237   0.0   2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
       "4    0.06905   0.0   2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
       "..       ...   ...    ...  ...    ...    ...   ...     ...  ...    ...   ...   \n",
       "494  0.06263   0.0  11.93  0.0  0.573  6.593  69.1  2.4786  1.0  273.0  21.0   \n",
       "495  0.04527   0.0  11.93  0.0  0.573  6.120  76.7  2.2875  1.0  273.0  21.0   \n",
       "496  0.06076   0.0  11.93  0.0  0.573  6.976  91.0  2.1675  1.0  273.0  21.0   \n",
       "497  0.10959   0.0  11.93  0.0  0.573  6.794  89.3  2.3889  1.0  273.0  21.0   \n",
       "498  0.04741   0.0  11.93  0.0  0.573  6.030  80.8  2.5050  1.0  273.0  21.0   \n",
       "\n",
       "         11    12    13  \n",
       "0    396.90  4.98  24.0  \n",
       "1    396.90  9.14  21.6  \n",
       "2    392.83  4.03  34.7  \n",
       "3    394.63  2.94  33.4  \n",
       "4    396.90  5.33  36.2  \n",
       "..      ...   ...   ...  \n",
       "494  391.99  9.67  22.4  \n",
       "495  396.90  9.08  20.6  \n",
       "496  396.90  5.64  23.9  \n",
       "497  393.45  6.48  22.0  \n",
       "498  396.90  7.88  11.9  \n",
       "\n",
       "[499 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = data.describe()\n",
    "outliers = []\n",
    "series = pd.Series([True]*len(data))\n",
    "for i in range(len(data.columns)):\n",
    "    seriesi = abs(data[0]-ds[0]['mean'])/ds[0]['std'] < 4\n",
    "    outliers.append( len(data) - seriesi.sum() )\n",
    "    series = series & seriesi\n",
    "print(outliers)\n",
    "print(series.sum())\n",
    "data1 = data[series].reset_index(drop=True)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce395d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.534426</td>\n",
       "      <td>0.276243</td>\n",
       "      <td>-1.272797</td>\n",
       "      <td>-0.274372</td>\n",
       "      <td>-0.129757</td>\n",
       "      <td>0.403283</td>\n",
       "      <td>-0.105890</td>\n",
       "      <td>0.125226</td>\n",
       "      <td>-0.970902</td>\n",
       "      <td>-0.650800</td>\n",
       "      <td>-1.442795</td>\n",
       "      <td>0.435186</td>\n",
       "      <td>-1.073789</td>\n",
       "      <td>0.139621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.530479</td>\n",
       "      <td>-0.491460</td>\n",
       "      <td>-0.578738</td>\n",
       "      <td>-0.274372</td>\n",
       "      <td>-0.725868</td>\n",
       "      <td>0.183129</td>\n",
       "      <td>0.380991</td>\n",
       "      <td>0.542237</td>\n",
       "      <td>-0.854581</td>\n",
       "      <td>-0.974339</td>\n",
       "      <td>-0.290794</td>\n",
       "      <td>0.435186</td>\n",
       "      <td>-0.479632</td>\n",
       "      <td>-0.124076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.530483</td>\n",
       "      <td>-0.491460</td>\n",
       "      <td>-0.578738</td>\n",
       "      <td>-0.274372</td>\n",
       "      <td>-0.725868</td>\n",
       "      <td>1.275323</td>\n",
       "      <td>-0.251599</td>\n",
       "      <td>0.542237</td>\n",
       "      <td>-0.854581</td>\n",
       "      <td>-0.974339</td>\n",
       "      <td>-0.290794</td>\n",
       "      <td>0.389133</td>\n",
       "      <td>-1.209474</td>\n",
       "      <td>1.315273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.529527</td>\n",
       "      <td>-0.491460</td>\n",
       "      <td>-1.291753</td>\n",
       "      <td>-0.274372</td>\n",
       "      <td>-0.820900</td>\n",
       "      <td>1.007993</td>\n",
       "      <td>-0.795342</td>\n",
       "      <td>1.062894</td>\n",
       "      <td>-0.738259</td>\n",
       "      <td>-1.094168</td>\n",
       "      <td>0.123927</td>\n",
       "      <td>0.409501</td>\n",
       "      <td>-1.365154</td>\n",
       "      <td>1.172436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.522630</td>\n",
       "      <td>-0.491460</td>\n",
       "      <td>-1.291753</td>\n",
       "      <td>-0.274372</td>\n",
       "      <td>-0.820900</td>\n",
       "      <td>1.220999</td>\n",
       "      <td>-0.496816</td>\n",
       "      <td>1.062894</td>\n",
       "      <td>-0.738259</td>\n",
       "      <td>-1.094168</td>\n",
       "      <td>0.123927</td>\n",
       "      <td>0.435186</td>\n",
       "      <td>-1.023800</td>\n",
       "      <td>1.480084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>-0.523837</td>\n",
       "      <td>-0.491460</td>\n",
       "      <td>0.129903</td>\n",
       "      <td>-0.274372</td>\n",
       "      <td>0.172618</td>\n",
       "      <td>0.429015</td>\n",
       "      <td>0.032711</td>\n",
       "      <td>-0.640902</td>\n",
       "      <td>-0.970902</td>\n",
       "      <td>-0.788604</td>\n",
       "      <td>1.183768</td>\n",
       "      <td>0.379628</td>\n",
       "      <td>-0.403935</td>\n",
       "      <td>-0.036177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-0.527101</td>\n",
       "      <td>-0.491460</td>\n",
       "      <td>0.129903</td>\n",
       "      <td>-0.274372</td>\n",
       "      <td>0.172618</td>\n",
       "      <td>-0.247173</td>\n",
       "      <td>0.302806</td>\n",
       "      <td>-0.731759</td>\n",
       "      <td>-0.970902</td>\n",
       "      <td>-0.788604</td>\n",
       "      <td>1.183768</td>\n",
       "      <td>0.435186</td>\n",
       "      <td>-0.488202</td>\n",
       "      <td>-0.233950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>-0.524188</td>\n",
       "      <td>-0.491460</td>\n",
       "      <td>0.129903</td>\n",
       "      <td>-0.274372</td>\n",
       "      <td>0.172618</td>\n",
       "      <td>0.976542</td>\n",
       "      <td>0.811010</td>\n",
       "      <td>-0.788812</td>\n",
       "      <td>-0.970902</td>\n",
       "      <td>-0.788604</td>\n",
       "      <td>1.183768</td>\n",
       "      <td>0.435186</td>\n",
       "      <td>-0.979524</td>\n",
       "      <td>0.128634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-0.515006</td>\n",
       "      <td>-0.491460</td>\n",
       "      <td>0.129903</td>\n",
       "      <td>-0.274372</td>\n",
       "      <td>0.172618</td>\n",
       "      <td>0.716360</td>\n",
       "      <td>0.750594</td>\n",
       "      <td>-0.683549</td>\n",
       "      <td>-0.970902</td>\n",
       "      <td>-0.788604</td>\n",
       "      <td>1.183768</td>\n",
       "      <td>0.396149</td>\n",
       "      <td>-0.859550</td>\n",
       "      <td>-0.080127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-0.526699</td>\n",
       "      <td>-0.491460</td>\n",
       "      <td>0.129903</td>\n",
       "      <td>-0.274372</td>\n",
       "      <td>0.172618</td>\n",
       "      <td>-0.375835</td>\n",
       "      <td>0.448515</td>\n",
       "      <td>-0.628350</td>\n",
       "      <td>-0.970902</td>\n",
       "      <td>-0.788604</td>\n",
       "      <td>1.183768</td>\n",
       "      <td>0.435186</td>\n",
       "      <td>-0.659593</td>\n",
       "      <td>-1.189853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.534426  0.276243 -1.272797 -0.274372 -0.129757  0.403283 -0.105890   \n",
       "1   -0.530479 -0.491460 -0.578738 -0.274372 -0.725868  0.183129  0.380991   \n",
       "2   -0.530483 -0.491460 -0.578738 -0.274372 -0.725868  1.275323 -0.251599   \n",
       "3   -0.529527 -0.491460 -1.291753 -0.274372 -0.820900  1.007993 -0.795342   \n",
       "4   -0.522630 -0.491460 -1.291753 -0.274372 -0.820900  1.220999 -0.496816   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "494 -0.523837 -0.491460  0.129903 -0.274372  0.172618  0.429015  0.032711   \n",
       "495 -0.527101 -0.491460  0.129903 -0.274372  0.172618 -0.247173  0.302806   \n",
       "496 -0.524188 -0.491460  0.129903 -0.274372  0.172618  0.976542  0.811010   \n",
       "497 -0.515006 -0.491460  0.129903 -0.274372  0.172618  0.716360  0.750594   \n",
       "498 -0.526699 -0.491460  0.129903 -0.274372  0.172618 -0.375835  0.448515   \n",
       "\n",
       "           7         8         9         10        11        12        13  \n",
       "0    0.125226 -0.970902 -0.650800 -1.442795  0.435186 -1.073789  0.139621  \n",
       "1    0.542237 -0.854581 -0.974339 -0.290794  0.435186 -0.479632 -0.124076  \n",
       "2    0.542237 -0.854581 -0.974339 -0.290794  0.389133 -1.209474  1.315273  \n",
       "3    1.062894 -0.738259 -1.094168  0.123927  0.409501 -1.365154  1.172436  \n",
       "4    1.062894 -0.738259 -1.094168  0.123927  0.435186 -1.023800  1.480084  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "494 -0.640902 -0.970902 -0.788604  1.183768  0.379628 -0.403935 -0.036177  \n",
       "495 -0.731759 -0.970902 -0.788604  1.183768  0.435186 -0.488202 -0.233950  \n",
       "496 -0.788812 -0.970902 -0.788604  1.183768  0.435186 -0.979524  0.128634  \n",
       "497 -0.683549 -0.970902 -0.788604  1.183768  0.396149 -0.859550 -0.080127  \n",
       "498 -0.628350 -0.970902 -0.788604  1.183768  0.435186 -0.659593 -1.189853  \n",
       "\n",
       "[499 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means, stds = data1.mean(axis=0), data1.std(axis=0)\n",
    "data1 = (data1-means)/stds\n",
    "data1 #.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f002b642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399, 14) (100, 14)\n",
      "(399, 13) (399, 1) (100, 13) (100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.52706949, -0.49145965, -1.13719333, -0.27437177, -0.80362134,\n",
       "        -0.21286324, -1.27866897,  0.97322529, -0.62193702,  0.15205586,\n",
       "        -0.70551401,  0.11462477, -0.48677363]),\n",
       " array([-0.3218492]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = list(range(len(data1)))\n",
    "random.shuffle(R)\n",
    "\n",
    "dataTrain, dataVal = data1.loc[R[:int(len(R)*0.8)]], data1.loc[R[int(len(R)*0.8):]]\n",
    "print(dataTrain.shape, dataVal.shape)\n",
    "xTrain, yTrain = np.array(dataTrain[ list(dataTrain.columns)[:13] ]), np.array(dataTrain[[13]])\n",
    "xVal, yVal = np.array(dataVal[ list(dataVal.columns)[:13] ]), np.array(dataVal[[13]])\n",
    "print(xTrain.shape, yTrain.shape, xVal.shape, yVal.shape)\n",
    "xTrain[0], yTrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e104b",
   "metadata": {},
   "source": [
    "### Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91fb8283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.5271, -0.4915, -1.1372, -0.2744, -0.8036, -0.2129, -1.2787,  0.9732,\n",
      "        -0.6219,  0.1521, -0.7055,  0.1146, -0.4868], device='cuda:0'), tensor([-0.3218], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        assert len(self.X), len(self.y)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.Tensor(self.X[index]).to('cuda')\n",
    "        y = torch.Tensor(self.y[index]).to('cuda')\n",
    "        return x,y\n",
    "    \n",
    "trainDataset = MyDataset(xTrain, yTrain)\n",
    "valDataset = MyDataset(xVal, yVal)\n",
    "print(trainDataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c08f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 7\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=batch_size, pin_memory=False)\n",
    "valDataLoader = DataLoader(valDataset, batch_size=batch_size, pin_memory=False)\n",
    "# stepPerEpoch = len(trainDataLoader)\n",
    "print( len(trainDataLoader), len(valDataLoader) ) # 399/16=24 # 100/16=6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad1c1b",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9386dc12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (dense1): Linear(in_features=13, out_features=64, bias=True)\n",
      "  (dense2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (dense3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "[('dense1.weight', torch.Size([64, 13]), 832), ('dense1.bias', torch.Size([64]), 64), ('dense2.weight', torch.Size([64, 64]), 4096), ('dense2.bias', torch.Size([64]), 64), ('dense3.weight', torch.Size([1, 64]), 64), ('dense3.bias', torch.Size([1]), 1)]\n",
      "[('dense1.weight', torch.Size([64, 13]), 832), ('dense1.bias', torch.Size([64]), 64), ('dense2.weight', torch.Size([64, 64]), 4096), ('dense2.bias', torch.Size([64]), 64), ('dense3.weight', torch.Size([1, 64]), 64), ('dense3.bias', torch.Size([1]), 1)]\n",
      "0.00209 tensor([[0.2049],\n",
      "        [0.1705]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1  = nn.Linear(13, 64)\n",
    "        self.dense2  = nn.Linear(64, 64)\n",
    "        self.dense3  = nn.Linear(64, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel().to('cuda')\n",
    "print( model )\n",
    "\n",
    "D = model.state_dict()\n",
    "print( [(name,D[name].shape,D[name].numel()) for name in D] )\n",
    "P = model.named_parameters()\n",
    "print( [(name,weight.shape,weight.numel()) for name,weight in P] )\n",
    "\n",
    "with torch.no_grad():\n",
    "    testInput = torch.rand(2,13).to('cuda')\n",
    "    start = time.time()\n",
    "    pred = model(testInput)\n",
    "    print( round(time.time()-start,5), pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1528d2f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.0694], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print( list(model.named_parameters())[-1][1] ) # last layer # weight # every time will different\n",
    "torch.save({'model_state_dict':model.state_dict(),'epoch':0,'loss':99}, 'init.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0371804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.0083], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0694], device='cuda:0', requires_grad=True) 0 99\n"
     ]
    }
   ],
   "source": [
    "model = MyModel().to('cuda')\n",
    "print( list(model.named_parameters())[-1][1] )\n",
    "\n",
    "checkpoint = torch.load('./init.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "print( list(model.named_parameters())[-1][1], epoch, loss )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092da9f4",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "774871f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be50c87",
   "metadata": {},
   "source": [
    "### Loss / Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20bd7c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6667)\n"
     ]
    }
   ],
   "source": [
    "lossFunc = nn.MSELoss()\n",
    "xReal = torch.Tensor([[0,1,2],[3,4,5]])\n",
    "xPred = torch.Tensor([[0,1,2],[3,4,3]])\n",
    "\n",
    "loss = lossFunc(xReal, xPred) # (5-3)**2/6\n",
    "print( loss )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e13a7",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecf437ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n",
      "batch=7/7, valLoss=0.051724449\n",
      "Epoch: 2/100\n",
      "batch=7/7, valLoss=0.041413300\n",
      "Epoch: 3/100\n",
      "batch=7/7, valLoss=0.036002788\n",
      "Epoch: 4/100\n",
      "batch=7/7, valLoss=0.032522517\n",
      "Epoch: 5/100\n",
      "batch=7/7, valLoss=0.030042328\n",
      "Epoch: 6/100\n",
      "batch=7/7, valLoss=0.028222187\n",
      "Epoch: 7/100\n",
      "batch=7/7, valLoss=0.026882081\n",
      "Epoch: 8/100\n",
      "batch=7/7, valLoss=0.025912002\n",
      "Epoch: 9/100\n",
      "batch=7/7, valLoss=0.025211942\n",
      "Epoch: 10/100\n",
      "batch=7/7, valLoss=0.024701896\n",
      "Epoch: 11/100\n",
      "batch=7/7, valLoss=0.024311860\n",
      "Epoch: 12/100\n",
      "batch=7/7, valLoss=0.024021831\n",
      "Epoch: 13/100\n",
      "batch=7/7, valLoss=0.023801806\n",
      "Epoch: 14/100\n",
      "batch=7/7, valLoss=0.023611785\n",
      "Epoch: 15/100\n",
      "batch=7/7, valLoss=0.023461767\n",
      "Epoch: 16/100\n",
      "batch=7/7, valLoss=0.023321751\n",
      "Epoch: 17/100\n",
      "batch=7/7, valLoss=0.023201737\n",
      "Epoch: 18/100\n",
      "batch=7/7, valLoss=0.023101724\n",
      "Epoch: 19/100\n",
      "batch=7/7, valLoss=0.023001713\n",
      "Epoch: 20/100\n",
      "batch=7/7, valLoss=0.022901702\n",
      "Epoch: 21/100\n",
      "batch=7/7, valLoss=0.022811693\n",
      "Epoch: 22/100\n",
      "batch=7/7, valLoss=0.022731684\n",
      "Epoch: 23/100\n",
      "batch=7/7, valLoss=0.022651676\n",
      "Epoch: 24/100\n",
      "batch=7/7, valLoss=0.022581669\n",
      "Epoch: 25/100\n",
      "batch=7/7, valLoss=0.022511662\n",
      "Epoch: 26/100\n",
      "batch=7/7, valLoss=0.022451656\n",
      "Epoch: 27/100\n",
      "batch=7/7, valLoss=0.022391651\n",
      "Epoch: 28/100\n",
      "batch=7/7, valLoss=0.022331645\n",
      "Epoch: 29/100\n",
      "batch=7/7, valLoss=0.022281641\n",
      "Epoch: 30/100\n",
      "batch=7/7, valLoss=0.022231637\n",
      "Epoch: 31/100\n",
      "batch=7/7, valLoss=0.022191633\n",
      "Epoch: 32/100\n",
      "batch=7/7, valLoss=0.022151629\n",
      "Epoch: 33/100\n",
      "batch=7/7, valLoss=0.022111626\n",
      "Epoch: 34/100\n",
      "batch=7/7, valLoss=0.022071623\n",
      "Epoch: 35/100\n",
      "batch=7/7, valLoss=0.022041621\n",
      "Epoch: 36/100\n",
      "batch=7/7, valLoss=0.022011618\n",
      "Epoch: 37/100\n",
      "batch=7/7, valLoss=0.021991616\n",
      "Epoch: 38/100\n",
      "batch=7/7, valLoss=0.021961614\n",
      "Epoch: 39/100\n",
      "batch=7/7, valLoss=0.021941612\n",
      "Epoch: 40/100\n",
      "batch=7/7, valLoss=0.021921611\n",
      "Epoch: 41/100\n",
      "batch=7/7, valLoss=0.021901609\n",
      "Epoch: 42/100\n",
      "batch=7/7, valLoss=0.021881608\n",
      "Epoch: 43/100\n",
      "batch=7/7, valLoss=0.021871607\n",
      "Epoch: 44/100\n",
      "batch=7/7, valLoss=0.021861606\n",
      "Epoch: 45/100\n",
      "batch=7/7, valLoss=0.021841605\n",
      "Epoch: 46/100\n",
      "batch=7/7, valLoss=0.021831604\n",
      "Epoch: 47/100\n",
      "batch=7/7, valLoss=0.021821603\n",
      "Epoch: 48/100\n",
      "batch=7/7, valLoss=0.021811602\n",
      "Epoch: 49/100\n",
      "batch=7/7, valLoss=0.021811601\n",
      "Epoch: 50/100\n",
      "batch=7/7, valLoss=0.021801601\n",
      "Epoch: 51/100\n",
      "batch=7/7, valLoss=0.021791600\n",
      "Epoch: 52/100\n",
      "batch=7/7, valLoss=0.021791599\n",
      "Epoch: 53/100\n",
      "batch=7/7, valLoss=0.021781599\n",
      "Epoch: 54/100\n",
      "batch=7/7, valLoss=0.021781598\n",
      "Epoch: 55/100\n",
      "batch=7/7, valLoss=0.021781598\n",
      "Epoch: 56/100\n",
      "batch=7/7, valLoss=0.021771597\n",
      "Epoch: 57/100\n",
      "batch=7/7, valLoss=0.021771597\n",
      "Epoch: 58/100\n",
      "batch=7/7, valLoss=0.021771597\n",
      "Epoch: 59/100\n",
      "batch=7/7, valLoss=0.021761596\n",
      "Epoch: 60/100\n",
      "batch=7/7, valLoss=0.021761596\n",
      "Epoch: 61/100\n",
      "batch=7/7, valLoss=0.021761596\n",
      "Epoch: 62/100\n",
      "batch=7/7, valLoss=0.021761596\n",
      "Epoch: 63/100\n",
      "batch=7/7, valLoss=0.021761595\n",
      "Epoch: 64/100\n",
      "batch=7/7, valLoss=0.021761595\n",
      "Epoch: 65/100\n",
      "batch=7/7, valLoss=0.021761595\n",
      "Epoch: 66/100\n",
      "batch=7/7, valLoss=0.021761595\n",
      "Epoch: 67/100\n",
      "batch=7/7, valLoss=0.021761594\n",
      "Epoch: 68/100\n",
      "batch=7/7, valLoss=0.021761594\n",
      "Epoch: 69/100\n",
      "batch=7/7, valLoss=0.021761594\n",
      "Epoch: 70/100\n",
      "batch=7/7, valLoss=0.021761594\n",
      "Epoch: 71/100\n",
      "batch=7/7, valLoss=0.021761594\n",
      "Epoch: 72/100\n",
      "batch=7/7, valLoss=0.021761593\n",
      "Epoch: 73/100\n",
      "batch=7/7, valLoss=0.021761593\n",
      "Epoch: 74/100\n",
      "batch=7/7, valLoss=0.021761593\n",
      "Epoch: 75/100\n",
      "batch=7/7, valLoss=0.021761593\n",
      "Epoch: 76/100\n",
      "batch=7/7, valLoss=0.021761593\n",
      "Epoch: 77/100\n",
      "batch=7/7, valLoss=0.021761593\n",
      "Epoch: 78/100\n",
      "batch=7/7, valLoss=0.021761593\n",
      "Epoch: 79/100\n",
      "batch=7/7, valLoss=0.021761593\n",
      "Epoch: 80/100\n",
      "batch=7/7, valLoss=0.021761592\n",
      "Epoch: 81/100\n",
      "batch=7/7, valLoss=0.021771592\n",
      "Epoch: 82/100\n",
      "batch=7/7, valLoss=0.021771592\n",
      "Epoch: 83/100\n",
      "batch=7/7, valLoss=0.021771592\n",
      "Epoch: 84/100\n",
      "batch=7/7, valLoss=0.021771592\n",
      "Epoch: 85/100\n",
      "batch=7/7, valLoss=0.021771592\n",
      "Epoch: 86/100\n",
      "batch=7/7, valLoss=0.021771592\n",
      "Epoch: 87/100\n",
      "batch=7/7, valLoss=0.021771592\n",
      "Epoch: 88/100\n",
      "batch=7/7, valLoss=0.021771592\n",
      "Epoch: 89/100\n",
      "batch=7/7, valLoss=0.021771592\n",
      "Epoch: 90/100\n",
      "batch=7/7, valLoss=0.021781591\n",
      "Epoch: 91/100\n",
      "batch=7/7, valLoss=0.021781591\n",
      "Epoch: 92/100\n",
      "batch=7/7, valLoss=0.021781591\n",
      "Epoch: 93/100\n",
      "batch=7/7, valLoss=0.021781591\n",
      "Epoch: 94/100\n",
      "batch=7/7, valLoss=0.021781591\n",
      "Epoch: 95/100\n",
      "batch=7/7, valLoss=0.021781591\n",
      "Epoch: 96/100\n",
      "batch=7/7, valLoss=0.021781591\n",
      "Epoch: 97/100\n",
      "batch=7/7, valLoss=0.021781591\n",
      "Epoch: 98/100\n",
      "batch=7/7, valLoss=0.021781591\n",
      "Epoch: 99/100\n",
      "batch=7/7, valLoss=0.021791591\n",
      "Epoch: 100/100\n",
      "batch=7/7, valLoss=0.021791591\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "output_dir = \".\"\n",
    "save_per_ep = 30\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "history = {\"trainLossL\":[], \"valLossL\":[]}\n",
    "for ep in range(epochs):\n",
    "    print(f\"Epoch: {ep+1}/{epochs}\")\n",
    "    # train\n",
    "    trainLoss= 0.\n",
    "    for i,(X,y) in enumerate(trainDataLoader):\n",
    "        optimizer.zero_grad()    # zero the parameter gradients\n",
    "        pred = model(X)          # predict\n",
    "        loss = lossFunc(y,pred)  # loss\n",
    "        loss.backward()          # calculate loss to update model.grad\n",
    "        optimizer.step()         # update parameters\n",
    "        trainLoss += loss.item() / len(trainDataset)\n",
    "        print(f\"\\rbatch={i+1}/{len(trainDataLoader)}, trainLoss={trainLoss:.5f}\", end=\"\")\n",
    "    history[\"trainLossL\"].append(trainLoss)\n",
    "    # save\n",
    "    if ep%save_per_ep==0 or ep==epochs-1:\n",
    "        torch.save({'model_state_dict':model.state_dict(),'epoch':ep,'loss':trainLoss}, f\"{output_dir}/ckpt-{ep}.pth\")\n",
    "    # validation\n",
    "    valLoss = 0.\n",
    "    for i,(X,y) in enumerate(valDataLoader):\n",
    "        with torch.no_grad():\n",
    "            pred = model(X)\n",
    "            loss = lossFunc(pred,y)\n",
    "            valLoss += loss.item() / len(valDataset)\n",
    "            print(f\"\\rbatch={i+1}/{len(valDataLoader)}, valLoss={valLoss:.5f}\", end=\"\")\n",
    "    history[\"valLossL\"].append(valLoss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a06f73",
   "metadata": {},
   "source": [
    "### Training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94e4a6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure()\n",
    "f.set_facecolor(\"orange\")\n",
    "plt.plot(history['trainLossL'])\n",
    "plt.plot(history['valLossL'])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"mse\")\n",
    "plt.legend(labels=['train','val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f768ea",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45068d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2721 10.4303 0.7166\n",
      "36.3721 17.414 -0.0415\n",
      "4.7721 5.2512 0.9737\n",
      "6.5721 8.7553 0.8801\n",
      "-6.4279 -4.0855 0.8713\n",
      "18.7721 20.9106 0.8825\n",
      "10.6721 15.2606 0.7479\n",
      "22.5721 14.3533 0.5485\n",
      "11.1721 12.4727 0.9285\n",
      "36.3721 29.4099 0.6175\n",
      "-2.1279 0.7754 0.8405\n",
      "21.4721 21.5683 0.9947\n",
      "3.0721 7.0551 0.7812\n",
      "10.5721 11.3475 0.9574\n",
      "11.3721 9.0059 0.87\n",
      "7.7721 7.9417 0.9907\n",
      "0.8721 0.7024 0.9907\n",
      "3.0721 5.8815 0.8457\n",
      "2.0721 1.0693 0.9449\n",
      "-5.5279 -9.4988 0.7819\n",
      "-0.3279 6.6062 0.6191\n",
      "-4.1279 -0.4099 0.7957\n",
      "7.8721 10.0283 0.8815\n",
      "6.6721 8.7731 0.8846\n",
      "5.1721 7.5467 0.8695\n",
      "-2.6279 0.7726 0.8132\n",
      "-3.4279 2.5535 0.6714\n",
      "8.5721 10.6772 0.8844\n",
      "-4.8279 -10.3302 0.6977\n",
      "9.4721 10.7467 0.93\n",
      "6.9721 7.6156 0.9646\n",
      "14.5721 19.2934 0.7406\n",
      "11.0721 9.3777 0.9069\n",
      "8.7721 9.963 0.9346\n",
      "5.4721 6.0624 0.9676\n",
      "23.5721 19.3368 0.7673\n",
      "3.8721 3.635 0.987\n",
      "19.4721 19.0128 0.9748\n",
      "0.2721 0.2144 0.9968\n",
      "1.9721 2.4866 0.9717\n",
      "9.4721 11.3054 0.8993\n",
      "5.7721 9.7438 0.7818\n",
      "-0.6279 3.7109 0.7616\n",
      "4.7721 2.2685 0.8625\n",
      "7.1721 4.7793 0.8685\n",
      "3.1721 5.3085 0.8826\n",
      "8.3721 12.5626 0.7698\n",
      "23.9721 23.9129 0.9967\n",
      "-2.3279 -0.0052 0.8724\n",
      "11.1721 13.3091 0.8826\n",
      "5.7721 3.8571 0.8948\n",
      "4.9721 6.5411 0.9138\n",
      "30.3721 23.795 0.6387\n",
      "12.9721 14.2454 0.93\n",
      "4.1721 4.3251 0.9916\n",
      "0.7721 -3.8354 0.7469\n",
      "19.5721 20.4746 0.9504\n",
      "1.5721 2.6333 0.9417\n",
      "33.0721 21.5548 0.3673\n",
      "21.5721 20.2839 0.9292\n",
      "5.6721 7.7495 0.8859\n",
      "10.2721 14.0375 0.7931\n",
      "0.6721 3.5372 0.8426\n",
      "11.1721 17.2786 0.6645\n",
      "36.3721 10.8203 -0.4037\n",
      "6.9721 13.423 0.6456\n",
      "6.6721 5.9435 0.96\n",
      "10.0721 13.2952 0.8229\n",
      "6.7721 6.0921 0.9626\n",
      "6.7721 7.5167 0.9591\n",
      "12.7721 9.5506 0.823\n",
      "12.5721 10.4045 0.8809\n",
      "36.3721 25.0455 0.3778\n",
      "9.1721 10.7202 0.915\n",
      "1.5721 -1.5811 0.8268\n",
      "8.6721 13.6864 0.7245\n",
      "5.2721 2.2758 0.8354\n",
      "5.7721 3.7077 0.8866\n",
      "7.1721 9.7353 0.8592\n",
      "36.3721 27.1007 0.4907\n",
      "36.3721 22.7375 0.251\n",
      "3.4721 6.5136 0.8329\n",
      "10.2721 14.2955 0.779\n",
      "13.0721 18.2793 0.7139\n",
      "3.9721 3.1107 0.9527\n",
      "7.5721 7.7009 0.9929\n",
      "2.9721 2.249 0.9603\n",
      "8.6721 12.8814 0.7688\n",
      "3.4721 5.8123 0.8714\n",
      "7.7721 8.6546 0.9515\n",
      "8.3721 14.0225 0.6896\n",
      "5.4721 3.0674 0.8679\n",
      "29.1721 16.023 0.2776\n",
      "12.7721 15.2143 0.8658\n",
      "5.8721 6.7096 0.954\n",
      "11.3721 13.9068 0.8608\n",
      "-0.2279 -0.2103 0.999\n",
      "4.1721 8.7933 0.7461\n",
      "10.3721 16.5211 0.6622\n",
      "0.4721 2.3876 0.8948\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model.eval()            # Turn off some layers such as dropout, batch normalization for model inference\n",
    "with torch.no_grad():   # Turn off gradient calculation\n",
    "    pass\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    realL, predL = [], []\n",
    "    for X,y in valDataLoader:\n",
    "        realL += list( y.cpu().detach().numpy().reshape(-1) )\n",
    "        pred = model(X)\n",
    "        predL += list( pred.cpu().detach().numpy().reshape(-1) )\n",
    "        \n",
    "realL = list(map(lambda x:round(x*stds[13]+stds[13],4),realL))\n",
    "predL = list(map(lambda x:round(x*stds[13]+stds[13],4),predL))\n",
    "errL  = [ round(1-abs(pred-real)/(stds[13]*2),4) for real,pred in zip(realL,predL) ]\n",
    "for real,pred,err in zip(realL,predL,errL):\n",
    "    print(real,pred,err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0faaecb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8091"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(sum(errL)/len(errL),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8dd7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99522d64",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dfb0341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (dense1): Linear(in_features=13, out_features=64, bias=True)\n",
      "  (dense2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (dense3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1  = nn.Linear(13, 64)\n",
    "        self.dense2  = nn.Linear(64, 64)\n",
    "        self.dense3  = nn.Linear(64, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel().to('cuda')\n",
    "print( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "714be99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./ckpt-99.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50c261e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8091"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    realL, predL = [], []\n",
    "    for X,y in valDataLoader:\n",
    "        realL += list( y.cpu().detach().numpy().reshape(-1) )\n",
    "        pred = model(X)\n",
    "        predL += list( pred.cpu().detach().numpy().reshape(-1) )\n",
    "        \n",
    "realL = list(map(lambda x:round(x*stds[13]+stds[13],4),realL))\n",
    "predL = list(map(lambda x:round(x*stds[13]+stds[13],4),predL))\n",
    "errL  = [ round(1-abs(pred-real)/(stds[13]*2),4) for real,pred in zip(realL,predL) ]\n",
    "#for real,pred,err in zip(realL,predL,errL):\n",
    "#    print(real,pred,err)\n",
    "round(sum(errL)/len(errL),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ada95c",
   "metadata": {},
   "source": [
    "### More:\n",
    "+ modularization all processes\n",
    "+ specify the weight, then load it after initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e61c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
